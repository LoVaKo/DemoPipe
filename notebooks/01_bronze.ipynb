{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2705259a",
   "metadata": {},
   "source": [
    "# Bronze Layer: Raw Data Ingestion\n",
    "üéØ **Goal**: Collect and store the raw data exactly as it comes in\n",
    "\n",
    "üîß **Tasks a data engineer performs:**\n",
    "- Write Python or Spark code to pull data from a source (API, file, database)\n",
    "- Validate basic connectivity (check API responses, handle errors)\n",
    "- Convert the raw response (e.g. JSON) to a DataFrame\n",
    "- Store the data in a raw Delta table (or Parquet/CSV in simpler setups)\n",
    "- Include basic metadata (e.g. ingestion timestamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff12b10",
   "metadata": {},
   "source": [
    "### üì• This notebook: Bronze Layer ‚Äì Raw Pok√©mon Data Ingestion\n",
    "\n",
    "In this notebook, we:\n",
    "\n",
    "- Collect data from the [Pok√©API](https://pokeapi.co/) for the first 150 Pok√©mon\n",
    "- Extract selected fields from the JSON response:\n",
    "  - `id`, `name`, `height`, `weight`, `base_experience`, and `types`\n",
    "- Convert the API responses into a Spark DataFrame\n",
    "- Save the raw data to a Delta table in the **bronze layer** at `./data/bronze/pokemon`\n",
    "\n",
    "üü´ No transformations are applied ‚Äî we store the data as-is to preserve its original structure.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841f5737",
   "metadata": {},
   "source": [
    "## üîß What is Spark? What is Delta Lake?\n",
    "\n",
    "### üí• Spark\n",
    "Apache Spark is a powerful engine that helps process large amounts of data quickly ‚Äî like a **supercharged calculator** for data.\n",
    "\n",
    "- It can read, transform, and analyze data stored in many formats (CSV, JSON, Parquet, Delta, etc.)\n",
    "- Spark works **in memory**, meaning it's much faster than traditional tools (like Excel or plain Python) when dealing with big data\n",
    "- You write code in Python (using **PySpark**) and Spark takes care of the heavy lifting behind the scenes\n",
    "\n",
    "Even though we‚Äôre working with small data here (like 150 Pok√©mon), Spark is used by **big companies** for tasks involving **millions or billions of rows**.\n",
    "\n",
    "---\n",
    "\n",
    "### üßä Delta Lake\n",
    "Delta Lake is a special file format built on top of **Parquet**, but with superpowers:\n",
    "\n",
    "- ‚úÖ Allows **versioning** ‚Äî like a time machine for your data\n",
    "- ‚úÖ Guarantees **data reliability** ‚Äî no broken writes or half-finished tables\n",
    "- ‚úÖ Supports updates, deletes, merges ‚Äî just like a database\n",
    "- ‚úÖ Works great with Spark\n",
    "\n",
    "In this project, we use **Delta tables** to store each stage of our data pipeline: `bronze`, `silver`, and `gold`. They're just folders on your computer that Spark reads/writes like structured tables.\n",
    "\n",
    "> üí° Think of Delta Lake as a smart format for storing big data ‚Äî reliable, efficient, and made for analytics or machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb794428",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. Initializing Spark with Delta support\n",
    "In this step, we set up a Spark session and enable Delta Lake functionality.\n",
    "\n",
    "- `SparkSession` is the main entry point to PySpark.\n",
    "- We configure Spark to understand and work with the Delta format by adding two `.config(...)` lines.\n",
    "- Delta Lake allows us to write data to disk in a way that's **reliable, versioned, and scalable**.\n",
    "\n",
    "This session powers everything we do in the pipeline from this point on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b89c75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Define the Spark session with Delta lake support\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DemoPipe - Bronze Layer\") \\\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-spark_2.12:3.3.0\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Confirm Spark is running\n",
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3818cf9",
   "metadata": {},
   "source": [
    "# Step 2: Set the path to your bronze Delta table\n",
    "We define a local path to store our **raw Pok√©mon data**.\n",
    "\n",
    "- The output path will be used by Spark to save data in **Delta format**\n",
    "- Spark will automatically create the folder and files if they don‚Äôt exist\n",
    "- This keeps our code flexible and easy to migrate (e.g., to cloud paths later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4558f7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define path to bronze table\n",
    "bronze_path = \"../data/bronze/pokemon\"\n",
    "\n",
    "# For Databricks (uncomment when running in DBFS)\n",
    "# bronze_path = \"dbfs:/tmp/bronze/pokemon\"\n",
    "\n",
    "# For Microsoft Fabric (Lakehouse managed table)\n",
    "# bronze_path = \"Tables/bronze_pokemon\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85aeebb1",
   "metadata": {},
   "source": [
    "# Step 3: Scrape Pok√©mon data from the Pok√©API\n",
    "\n",
    "We fetch raw data for the first three generations of Pok√©mon using the public [Pok√©API](https://pokeapi.co).\n",
    "\n",
    "- We use `requests` to make HTTP calls and collect JSON data\n",
    "- From each Pok√©mon, we COULD just extract key fields, but to simulate how messy data can be, let's just scrape the raw JSON file in it's entirety.\n",
    "- We store this data in a list of dictionaries to convert it easily into a DataFrame\n",
    "\n",
    "This step represents the **real-world ‚Äúingest‚Äù phase** in a data pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f14216",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Scrape all available fields for the first three generations of Pok√©mon\n",
    "pokemon_data = []\n",
    "\n",
    "for i in range(1, 387):\n",
    "    response = requests.get(f\"https://pokeapi.co/api/v2/pokemon/{i}\")\n",
    "    if response.status_code == 200:\n",
    "        raw_json = response.json()\n",
    "        pokemon_data.append({\n",
    "            \"id\": raw_json[\"id\"],\n",
    "            \"name\": raw_json[\"name\"],\n",
    "            \"raw_json\": json.dumps(raw_json)  # üí° Store full response here\n",
    "        })\n",
    "\n",
    "# Convert to a Pandas DataFrame\n",
    "pdf = pd.DataFrame(pokemon_data)\n",
    "\n",
    "# Preview the structure\n",
    "pdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27804f3",
   "metadata": {},
   "source": [
    "# Step 4: Convert to Spark DataFrame\n",
    "After scraping, we:\n",
    "\n",
    "- Convert the list of dictionaries into a Pandas DataFrame\n",
    "- Then convert the Pandas DataFrame into a Spark DataFrame\n",
    "\n",
    "This allows us to use all the power of Spark to process and save the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75f21da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Spark DataFrame\n",
    "df = spark.createDataFrame(pdf)\n",
    "\n",
    "# Show the schema and first few rows\n",
    "df.printSchema()\n",
    "df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407d1d38",
   "metadata": {},
   "source": [
    "# Step 5: Save to Delta table in the bronze layer\n",
    "Now we save the raw data to a **Delta table** on local disk.\n",
    "\n",
    "- This data is stored in `../data/bronze/pokemon` using the Delta format\n",
    "- Delta adds features like versioning, schema enforcement, and safe overwrites\n",
    "\n",
    "At this stage, we do **not** clean or modify the data ‚Äî we store it exactly as it was received."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeee34a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to local Delta table (bronze)\n",
    "df.write.format(\"delta\").mode(\"overwrite\").save(bronze_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105ae7bf",
   "metadata": {},
   "source": [
    "# Optional: Confirm the Delta table saved correctly\n",
    "We read the data back from disk to confirm that:\n",
    "\n",
    "- The Delta format was written properly\n",
    "- The data matches our expectations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5925b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read back from the bronze path to verify\n",
    "bronze_df = spark.read.format(\"delta\").load(bronze_path)\n",
    "bronze_df.show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo-pipeline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
